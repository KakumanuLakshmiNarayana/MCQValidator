{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**CNN ARCHITECTURE AND TRAINING**"
      ],
      "metadata": {
        "id": "Mi-4Q1xaIpTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# Function to filter the dataset\n",
        "def filter_indices(dataset, classes):\n",
        "    indices = []\n",
        "    for i in range(len(dataset)):\n",
        "        if dataset.targets[i] in classes:\n",
        "            indices.append(i)\n",
        "    return indices\n",
        "\n",
        "# Function to adjust labels\n",
        "def adjust_labels(sample):\n",
        "    image, label = sample\n",
        "    return image, label - 1  # Subtract 1 to make labels start from 0\n",
        "\n",
        "# Step 3: Load and Preprocess the Data with Filter for Digits 1 to 4\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset_full = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset_full = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "# Filter the datasets to only include digits 1-4 (labels 1, 2, 3, 4)\n",
        "train_indices = filter_indices(train_dataset_full, [1, 2, 3, 4])\n",
        "test_indices = filter_indices(test_dataset_full, [1, 2, 3, 4])\n",
        "\n",
        "# Apply label adjustment to the datasets\n",
        "train_dataset = Subset(train_dataset_full, train_indices)\n",
        "train_dataset = [(adjust_labels(train_dataset[i])) for i in range(len(train_dataset))]\n",
        "\n",
        "test_dataset = Subset(test_dataset_full, test_indices)\n",
        "test_dataset = [(adjust_labels(test_dataset[i])) for i in range(len(test_dataset))]\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Step 4: Neural Network Model for 4 Output Classes\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(nn.functional.max_pool2d(self.conv1(x), 2))\n",
        "        x = nn.functional.relu(nn.functional.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return nn.functional.log_softmax(x, dim=1)\n",
        "\n",
        "model = Net()\n",
        "\n",
        "# Step 5: Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Step 6: Train the Model on Filtered Data\n",
        "for epoch in range(40):  # loop over the dataset multiple times\n",
        "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Step 7: Evaluate the Model on Filtered Data\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Accuracy: {100 * correct / total}%')\n",
        "\n",
        "# Step 8: Save the Model\n",
        "torch.save(model.state_dict(), 'mnist_model_1_to_4.pth')\n",
        "print(\"Model saved as mnist_model_1_to_4.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVTxmK2ljAeF",
        "outputId": "ccb611e4-13a0-481b-ea49-0e3262fdc8bd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 99.83169031017071%\n",
            "Model saved as mnist_model_1_to_4.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSTALLING ONNX**"
      ],
      "metadata": {
        "id": "UCTu4QlLJM7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxHUXyFGAJlL",
        "outputId": "8e46595d-68ff-45af-c64c-2558c9801aae"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CREATING ONNX FILE**"
      ],
      "metadata": {
        "id": "tKHbc_spJWwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "model = Net()\n",
        "model.load_state_dict(torch.load('mnist_model_1_to_4.pth', map_location=device))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
        "\n",
        "torch.onnx.export(model,\n",
        "                  dummy_input,\n",
        "                  \"mnist_model_1_to_4.onnx\",\n",
        "                  export_params=True,\n",
        "                  opset_version=10,\n",
        "                  do_constant_folding=True,\n",
        "                  input_names=['input'],\n",
        "                  output_names=['output'])"
      ],
      "metadata": {
        "id": "PANgFNngAGte"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CHECK IF ONNX FILE IS CREATED**"
      ],
      "metadata": {
        "id": "nRfzNHA1JeUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "onnx_model = onnx.load(\"mnist_model_1_to_4.onnx\")\n",
        "onnx.checker.check_model(onnx_model)"
      ],
      "metadata": {
        "id": "z8br7jXiASRl"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}